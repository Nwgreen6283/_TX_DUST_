---
title: "ENV_COP_DUST"
format:
  html:
    theme: default
---

**This code combines clean data sets (dust, copies, and env)**

# Getting Started

```{r}
here<-here::here
select<- dplyr::select
rename<- dplyr::rename
```

#### Libraries

```{r, include = FALSE}
library(tidyverse)
library(here)
library(Hmisc) #Lag Function
```

#### Load Clean Data

```{r, include = FALSE}
dust<- readRDS(here("data", "processed_data", "dust", "dust_final.rds")) #Clean dust data
copies<- read_csv(here("data", "processed_data", "qpcr", "total_clean_copies.csv")) #Clean Copies Data
env<- read_csv(here('data', 'processed_data','env', 'ENV_META_AVG.csv' )) #Clean Environmental Data
dust_perm<- read_csv(here("data", "processed_data", "dust", "dust_master.csv")) #Dust permutations
cfu<- read_rds(here("data", "processed_data", "cfu", "cfu.rds")) #CFU Data
hf<- read_csv(here("data", "processed_data", "qpcr", "hf183", "bo_hf183.csv")) #HF183
sp<- read_csv(here("data", "processed_data", "qpcr", "species_all.csv")) #Species
tot_bac<- read_csv(here("data", "processed_data", "qpcr", "total_bacteria.csv")) #Total Daily Bacteria
tot_bac_mon<- read_csv(here("data", "processed_data", "qpcr", "total_bacteria_monthly.csv")) #Total Monthly Bacteria
tss<- read_csv(here("data", "processed_data", "env", "tss.csv"))
```

# Further Cleaning

#### Drop NAs from Environmental Data

```{r}
env<- env %>%
  filter(!date == as.Date('2022-02-16') | !site == "RD") 
```

#### Combine Total Bacteria Datasets

```{r}
tot_bac<-
  rbind(tot_bac, tot_bac_mon)
```

<!--# NOTE: Va data was acyaully cleaned in species_cleaning.qmdSo i took it out here -->

#### Rename Va column

```{r}
#va_d<-
  #va_d %>%
  #rename("Va" = "copies_mL") %>%
  #select(date, site, Va)

#va_mon<-
  #va_mon %>%
  #rename("Va" = "copies_mL") %>%
  #select(date, site, Va)
```

#### Combine Va Monthly and Daily

```{r}
#va<- 
 # va_d %>% 
  #full_join(va_mon)
```

#### Cut Va out of old species DF

```{r}
#sp<-
  #sp %>%
 # select(!Va)
```

#### Combine data sets: (1) Dust, (2) Total Vib, (3) Environmental, (4) CFU, (5) Species

```{r}
master<- #env %>% 
  dust %>% full_join(copies)

master<-
  master %>%
  full_join(env)

master<-
  master %>%
  full_join(cfu)

master<-
  master %>%
  full_join(hf)

master<- 
  master %>%
  full_join(sp)

master<-
  master %>%
  full_join(tot_bac)

master<- 
  master %>%
  full_join(tss)

```

#### Create VAI

Create Cell Equivalents by dividing sample copies per mL by average 16S rDNA for Total Vibrio (9 copies) and Total Bacteria (3.5)

```{r}
# Total Vibrio CE
master <- 
  master %>%
  mutate(
    VAI = ((copies_mL / 9) / (tot_bacteria / 3.5))) %>%
  mutate(site = recode(site, "RD" = "Gulf",
                       "C2" = "Canals",
                       "BO" = "Blind Oso"))
  
```

#### Clean Data Again

We will get warnings since we have dust data for the whole year, but Nutrient and Copies_mL data for just the monthly and daily time series. This is ok.

#### Create Dust lag

```{r}
dust_perm<-
  dust_perm %>%
  mutate(dust_lag = lag(t7sum))
```

## DAILY Site-specific data frames

#### Trim Dates

```{r}
master2<-
  master %>%
  filter(date %in% as.Date('2022-07-05') : as.Date('2022-07-19'))
```

#### Add Dust permutations DF

```{r}
master2<-
  master2 %>% full_join(dust_perm) %>%
  filter(hr_cst %in% "01") %>% #Gets rid of duplicates
  select(!c(Dust, SUM, Type, t1, t7, t13, t19, tsum, tavg, t7avg, hr_cst)) %>% #Get rid of var
  rename("dust" = "t7sum")
```

#### Site-specific Dataframes + Lag

<!--# Note: C2 has no sig relationship. So we make lag all 1 so that correlation matrix does not get messed up and have two dust lag options (1 and 2 days) -->

```{r}
bo<- 
  master2 %>%
  filter(site %in% "Blind Oso"| date %in% c( as.Date("2022-07-05"), as.Date("2022-07-06"))) %>%
  select(!site) %>% 
  filter(date %in% as.Date('2022-07-07') : as.Date('2022-07-19')) 
c2<- 
  master2 %>%
  filter(site %in% "Canals"| date %in% c( as.Date("2022-07-05"), as.Date("2022-07-06"))) %>%
  select(!site) %>% 
  filter(date %in% as.Date('2022-07-07') : as.Date('2022-07-19')) 
rd<- 
  master2 %>%
  filter(site %in% "Gulf"| date %in% c( as.Date("2022-07-05"), as.Date("2022-07-06"))) %>%
  select(!site) %>% 
  filter(date %in% as.Date('2022-07-07') : as.Date('2022-07-19')) 
```

## MONTHLY Site-specific data frames

#### Select Monthly Dates

Also including 7/18/23 to represent July

```{r}
monthly<-
  master %>%
  filter(Type %in% "Monthly" | date %in% as.Date('2022-07-18'),
         hr_cst %in% c("00","01")) #Gets rid of duplicates
```

#### Site-specific DF

```{r}
bo_m<-
  monthly %>%
  filter(site %in% "Blind Oso") %>%
  select(!c(site, hr_cst))

c2_m<-
  monthly %>%
  filter(site %in% "Canals") %>%
  select(!c(site, hr_cst))

rd_m<-
  monthly %>%
  filter(site %in% "Gulf") %>%
  select(!c(site, hr_cst))
  
```

# Save CSV

```{r}
#MASTER
write.csv(master,file= here("data","processed_data","tx_master.csv")) 

#[DAILY]

#BLIND OSO
write.csv(bo,file= here("data","processed_data","bo_master_daily.csv"))

#CANALS
write.csv(c2,file= here("data","processed_data","c2_master_daily.csv"))

#GULF
write.csv(rd,file= here("data","processed_data","rd_master_daily.csv"))

#[MONTHLY]

#BLIND OSO
write.csv(bo_m,file= here("data","processed_data","bo_master_monthly.csv"))

#CANALS
write.csv(c2_m,file= here("data","processed_data","c2_master_monthly.csv"))

#GULF
write.csv(rd_m,file= here("data","processed_data","rd_master_monthly.csv"))
```
